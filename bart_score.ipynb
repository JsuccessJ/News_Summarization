{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94fc4580-9a68-4d99-86c2-5ace4070acf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig,PreTrainedTokenizerFast\n",
    "import torch\n",
    "\n",
    "\n",
    "# Specify the directory containing the model and tokenizer files\n",
    "model_dir = './model/bart1/checkpoint-133500'\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c0ab62-845b-4459-b380-06eed01758a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 결과: 쌀 생산조정제는 벼를 심었던 논에 벼 대신 사료작물이나 콩 등 다른 작물을 심으면 벼와의 일정 소득차를 보전해주는 제도다 올해 전남의 논 다른 작물 재배 계획면적은 전국 5만ha의\n"
     ]
    }
   ],
   "source": [
    "# 문장에 사용된 특수 문자를 정리하는 함수\n",
    "def clean_special_characters(text):\n",
    "    special_characters = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    cleaned_text = ''.join(char for char in text if char not in special_characters)\n",
    "    return cleaned_text\n",
    "\n",
    "# 입력 문장\n",
    "satoori_text = \"\"\"전라남도가 쌀 과잉문제를 근본적으로 해결하기 위해 올해부터 시행하는 쌀 생산조정제를 적극 추진키로 했다. 쌀 생산조정제는 벼를 심었던 논에 벼 대신 사료작물이나 콩 등 다른 작물을 심으면 벼와의 일정 소득차를 보전해주는 제도다. 올해 전남의 논 다른 작물 재배 계획면적은 전국 5만ha의 약 21%인 1만 698ha로, 세부시행지침을 확정, 시군에 통보했다. 지원 대상 작물은 1년생을 포함한 다년생의 모든 작물이 해당되나 재배 면적 확대 시 수급과잉이 우려되는 고추, 무, 배추, 인삼, 대파 등 수급 불안 품목은 제외된다. 농지의 경우도 이미 다른 작물 재배 의무가 부여된 간척지, 정부매입비축농지, 농진청 시범사업, 경관보전 직불금 수령 농지 등은 제외될 예정이다. ha(3000평)당 지원 단가는 평균 340만원으로 사료작물 400만원, 일반작물은 340만원, 콩·팥 등 두류작물은 280만원 등이다. 논에 다른 작물 재배를 바라는 농가는 오는 22일부터 2월 28일까지 농지 소재지 읍면동사무소에 신청해야 한다. 전남도는 도와 시군에 관련 기관과 농가 등이 참여하는‘논 타작물 지원사업 추진협의회’를 구성, 지역 특성에 맞는 작목 선정 및 사업 심의 등을 본격 추진할 방침이다. 최향철 전라남도 친환경농업과장은 “최근 쌀값이 다소 상승추세에 있으나 매년 공급과잉에 따른 가격 하락으로 쌀농가에 어려움이 있었다”며“쌀 공급과잉을 구조적으로 해결하도록 논 타작물 재배 지원사업에 많이 참여해주길 바란다”고 말했다.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 입력 문장 정리\n",
    "cleaned_satoori_text = clean_special_characters(satoori_text)\n",
    "\n",
    "# Tokenize the cleaned input text\n",
    "input_ids = tokenizer(cleaned_satoori_text, return_tensors='pt').input_ids\n",
    "\n",
    "# Generate the translation\n",
    "translation_ids = model.generate(input_ids, max_length=50, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
    "\n",
    "# Decode the generated translation\n",
    "translated_text = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the translated text\n",
    "print(\"번역 결과:\", translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32182c02-961a-409f-8e01-8a89bd97d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "#라이브러리 총집합\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31fc7c2b-acc3-4c95-b323-391df4ed379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./document_summaries_test_utf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dc430b6-1c96-47df-9d39-ddf7247fe3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b4e54da-8ee9-4068-a78d-29a2b10e3c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 문장의 글자수: 1630\n",
      "가장 긴 문장을 출력함: 트럼프 대통령은 특히 협상이 타결된다면 미 워싱턴DC에서 시진핑 중국 국가주석과 미중 정상회담을 개최하겠다고 해, 이르면 이달 내 미중 정상이 만나 협상 타결을 선언할 수 있게 될지 주목된다. 트럼프 대통령은 이날 백악관에서 9번째 미중 통상협상에 참여한 류허 중국 부총리와 면담한 자리에서 협상 전망을 묻는 기자 질문에 \"먼저 무역합의부터 타결 짓고 시진핑 주석과 정상회담을 가질 것\"이라면서 \"우리는 아마도 4주 안에 알게 될 것이며, (전망이) 매우 좋아 보인다\"고 말했다. 이는 당초 트럼프 대통령이 미중 정상회담 일정을 발표할 것이란 예측을 깨고 '선 협상타결 후 정상회담' 전략을 고수한 것이다. 트럼프 대통령은 미중정상회담 장소는 워싱턴DC가 될 것이라고 밝혔다고 로이터통신이 전했다. 트럼프 대통령은 이어 \"우리는 중국과의 무역에서 한해에 최대 6000억달러까지 수년을 잃어왔기 때문에 좋은 딜을 타결해야 한다\"며 굿 딜이 아니면 노딜이 될 수밖에 없음을 강조했다. 트럼프 대통령은 그러나 \"중국이 강력하게 합의를 원하고 있고 큰 진전을 이뤄왔다\"며서 머지않아 최종 거래를 성사시킬 수 있을 것으로 기대했다. 트럼프 대통령은 또 \"갈 길이 남아있다\"면서도 협상이 타결된다면 그것은 \"기념비적 합의\"가 될 것이라고 말했다. 트럼프 대통령은 진전이 빠른 속도로 이뤄지고 있으며, 갈 길이 남아있지만 그다지 먼 길이 남아있는 것은 아니라고 말했다. 트럼프 대통령은 남은 협상 난제가 뭐냐는 질문을 받고는 '관세'와 '지적 재산권 탈취', '합의 이행' 문제를 꼽으며 류 부총리와 관세 문제를 논의할 것이라고 말했다. 류 부총리도 \"협상에서 엄청난 진전이 있었다\"고 밝혔다. 월스트리트 저널을 비롯한 미국 주요 언론은 미국과 중국이 9번이나 해온 마라톤 협상에서 거의 모든 쟁점들에서 합의에 도달했고 마지막 쟁점만 해결하면 양 정상이 만나 최종 타결하는 즉시 무역전쟁 끝내는 엔드게임에 돌입한 것으로 해석하고 있다. 아직 최종 타결짓지 못한 마지막 쟁점은 부과된 관세를 언제 철회하느냐는 문제인데 중국측은 합의서명 즉시 미국이 부과하고 있는 중국산 2500억달러 어치에 대한 관세를 모두 철폐해야한다고 요구하고 있는 반면, 미국은 중국측 잘못에 따른 징벌적 관세는 약속이행이 완료될 때까지 유지해야 한다고 맞서고 있다. 잠정합의한 부분을 보면 무역적자축소를 위해 중국은 대두와 에너지 상품 등 미국산 상품 구매를 약속한 만큼 늘리고 강제기술이전을 없애기 위해서는 중국시장에 진출한 미국 기업들이 지분을 100% 소유한 독자법인설립을 허용하며 더 나은 지적재산권 보호조치를 취하는 합의사항을 2025년까지 이행키로 했다. 미국은 중국의 불공정 경제정책으로 미국과의 교역에서 한해 3500억달러 이상의 흑자를 누리고 있다고 성토하며 지난해 3월부터 무역전쟁을 선포하고 7월 초부터 중국산 수입품 500억달러 어치에는 25%, 2000억달러 어치에는 10% 의 관세폭탄을 투하했다. 중국도 이에 맞서 미국산 수입품 전량인 1200억달러 어치에 10~25%의 차등관세를 부과해 왔다. 게다가 이번 무역협상이 결렬될 경우 미국은 2000억달러어치에 대한 관세를 현재 10%에서 25%로 2배 이상 높이도록 돼 있어 한해 교역규모가 6500~7000억달러에 달하는 슈퍼파워들의 전면전으로 비화될 위기에 빠져 있었다.\n"
     ]
    }
   ],
   "source": [
    "# 1열(첫 번째 열)에서 가장 긴 문자열의 길이 찾기\n",
    "max_length = data.iloc[:, 0].str.len().max()\n",
    "\n",
    "# 가장 긴 문자열 찾기\n",
    "longest_sentence = data.loc[data.iloc[:, 0].str.len() == max_length, data.columns[0]].values[0]\n",
    "\n",
    "print(\"가장 긴 문장의 글자수:\", max_length)\n",
    "print(\"가장 긴 문장을 출력함:\", longest_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "722e2607-1650-4944-88be-77bb997be4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# sumDataset 클래스 정의\n",
    "class evalDataset(Dataset):\n",
    "    def __init__(self, original, extractive, tokenizer):\n",
    "        self.original = original\n",
    "        self.extractive = extractive\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_text = self.original[idx]\n",
    "        extractive_text = self.extractive[idx]\n",
    "\n",
    "        # 토크나이징 및 패딩 수행\n",
    "        inputs = self.tokenizer(\n",
    "            text=original_text,\n",
    "            padding=\"max_length\",  # 필요한 경우 max_length 설정\n",
    "            truncation=True,       # 필요한 경우 문장 자르기 설정\n",
    "            return_tensors=\"pt\",   # PyTorch 텐서로 반환\n",
    "            max_length=512,\n",
    "            )\n",
    "\n",
    "        outputs = self.tokenizer(\n",
    "            text=extractive_text,\n",
    "            padding=\"max_length\",  # 필요한 경우 max_length 설정\n",
    "            truncation=True,       # 필요한 경우 문장 자르기 설정\n",
    "            return_tensors=\"pt\",   # PyTorch 텐서로 반환\n",
    "            max_length=512,\n",
    "            )\n",
    "\n",
    "        # 모델 입력을 포함하는 딕셔너리 반환\n",
    "        sample = {\n",
    "            'input_ids': inputs.input_ids.squeeze(),\n",
    "            'attention_mask': inputs.attention_mask.squeeze(),\n",
    "            'labels': outputs.input_ids.squeeze()  # 예시로 labels에 input_ids를 사용하도록 설정\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c049159-fe16-4e1a-8263-8b2d6b0b432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b08caa2-e318-4f84-ba44-f69e6b9a37e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 전처리\n",
    "\n",
    "# 1열(standard)과 2열(dialect)을 각각 features(X)와 labels(y)로 설정\n",
    "x = data['original']\n",
    "y = data['extractive']\n",
    "\n",
    "test_dataset = evalDataset(x, y, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ec128ab-acdb-4588-b43b-a1755d2f7c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        문 대통령은 \"5G가 각 분야에 융합되면, 정보통신산업을 넘어 자동차, 드론(무인항...\n",
      "1        손 대표는 회의를 주재하면서 \"의원들이나 지역위원장들, 당원들이 다음 선거에 대해 ...\n",
      "2        무대에 마련된 초고화질(4K UHD) 대형 스크린에서 국악기와 양악기가 함께 연주하...\n",
      "3        박 시장의 발언은 서울 내 노후 아파트 주민들이 서울시를 상대로 재건축 인허가를 요...\n",
      "4        SK와 알파벳은 신규 투자는 지주회사가 전담하고, 자회사는 본업에만 충실할 수 있도...\n",
      "                               ...                        \n",
      "29311    외국인 계절근로자들의 지역 체류와 근로의욕 고취를 위한 취지다. 베트남 국적의 근로...\n",
      "29312    국립과천과학관에서도 다음달 5일부터 '우주연구실 인턴 체험 특별전'을 연다. 대전에...\n",
      "29313    황 대표는 이날 김광림 최고위원과 영주시민회관에서 열린 한국당 영주문경예천 당원협의...\n",
      "29314    특히 경북형 사회적 경제 청년일자리사업 2억6700만 원, 사회적기업 사업개발비 3...\n",
      "29315    이들 4개 지구는 지난해 9월 수도권 30만 가구 주택공급 계획과 함께 1차 입지로...\n",
      "Name: original, Length: 29316, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d7de62c-38e1-49d8-9a1c-91a0cb076ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./testoutput/bart1\",\n",
    "    predict_with_generate=True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer 객체 생성 (훈련 부분 제거)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    eval_dataset = test_dataset,\n",
    ")\n",
    "\n",
    "# 테스트 데이터를 평가 데이터셋으로 설정\n",
    "test_dataset = evalDataset(x, y ,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f758e6a5-5a84-4c5b-b52e-460686af0e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([14111, 15606, 14050,   252,   270,  8981, 14319, 22331, 20550, 15848,\n",
      "          243, 23700, 14934, 12007, 15409, 15527,   243, 14340, 10341,   239,\n",
      "        10667, 12037, 16732,  9264, 14505, 20760,   243, 25788, 13679, 19541,\n",
      "        23937, 16152, 16794,   266, 16152, 15087, 16464, 20680,  9120, 20094,\n",
      "        11786, 23107, 14199, 14982, 21493, 28897, 15929, 17554, 11280, 14416,\n",
      "        14345, 19810, 14253, 14130, 14111, 15606, 27274, 24533, 16956, 25019,\n",
      "        14144,   270, 20527, 15093, 14508, 28451, 13842, 14490, 13607, 16785,\n",
      "        14990, 14511, 19754, 14111, 15606, 14370, 15592, 25053, 16745, 21039,\n",
      "        14199, 14144,   270, 18087, 20779, 22112, 15911,  9776, 14318, 14047,\n",
      "        12669, 15761, 19754,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([14552, 24472, 11936, 14680, 20170, 14063, 28623, 14144,   270, 15911,\n",
      "        20829, 15127, 14047, 12669, 13590, 16414, 15606, 14050,  9553, 14161,\n",
      "        14344, 12325, 14426, 14063, 15509, 21789, 16388, 14144,   270, 18554,\n",
      "        14050, 24838, 15119, 16061, 25903, 14667, 14570, 14351,  9904, 14254,\n",
      "        24784, 14276, 20029, 14111, 15606, 16635, 14245, 16653,  9115, 15359,\n",
      "        14739, 14144,   270, 20769, 15282, 21687, 11786, 19613, 14050,   252,\n",
      "          270, 14044, 15597, 15472, 14246, 14633, 14397, 10586, 17175, 14370,\n",
      "        18898, 12034, 14829, 16687, 18554, 14050,   252,   270,  9698, 16263,\n",
      "        16586, 11280, 14853, 19420, 16661, 14741, 15615, 14111, 15606, 14050,\n",
      "          252,   270,  8981, 14319, 22331, 20550, 15848,   243, 23700, 14934,\n",
      "        12007, 15409, 15527,   243, 14340, 10341,   239, 10667, 12037, 16732,\n",
      "         9264, 14505, 20760,   243, 25788, 13679, 19541, 23937, 16152, 16794,\n",
      "          266, 16152, 15087, 16464, 20680,  9120, 20094, 11786, 23107, 14199,\n",
      "        14982, 21493, 28897, 15929, 17554, 11280, 14416, 14345, 19810, 14253,\n",
      "        14130,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3])}\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 샘플 얻기\n",
    "first_sample = test_dataset[0]\n",
    "print(first_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcbc63e-50db-4869-a02d-729d5f760e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3091b75-708e-4e6c-b292-0b162a9ec31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3665' max='3665' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3665/3665 13:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhjs3545\u001b[0m (\u001b[33mnlp_learning\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\jupyter_notebook\\HJS\\summarization\\wandb\\run-20231211_133510-jh9sqlvd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_learning/huggingface/runs/jh9sqlvd' target=\"_blank\">young-wood-26</a></strong> to <a href='https://wandb.ai/nlp_learning/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_learning/huggingface' target=\"_blank\">https://wandb.ai/nlp_learning/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_learning/huggingface/runs/jh9sqlvd' target=\"_blank\">https://wandb.ai/nlp_learning/huggingface/runs/jh9sqlvd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20944644510746002, 'eval_rouge1': 0.1953, 'eval_rouge2': 0.061, 'eval_rougeL': 0.1927, 'eval_rougeLsum': 0.1927, 'eval_gen_len': 19.9236, 'eval_runtime': 811.0791, 'eval_samples_per_second': 36.144, 'eval_steps_per_second': 4.519}\n"
     ]
    }
   ],
   "source": [
    "# 평가 수행\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c900d-6512-4cbf-bfd1-c34ce29f3e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
